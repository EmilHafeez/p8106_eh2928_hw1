---
title: "P8106 Data Science Homework 1"
author: "Emil Hafeez"
geometry: "left=2.57cm,right=2.57cm,top=2.57cm,bottom=2.57cm"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

EMIL:
DO YOU NEED TO CV ON THE TEST DATASET TOO? FOR ALL PROBLEMS
DOES RIDGE REGRESSION REQUIRE THE X PASSED IN TO BE THE MATRIX SHE STANDARDIZES IN L5RMD?

There were missing values in resampled performance measures. for ridge reg

DO RMSE AT THE END USING RESAMPLE



# Section 1: Setup

## Libraries and Options

```{r libraries, include = F}
library(RNHANES)
library(tidyverse)
library(leaps)
library(ggplot2)
library(caret)
library(corrplot)
```

```{r opts chunk, include = F}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60))

knitr::opts_chunk$set(echo = TRUE)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
```

```{r input, message = F}
train_data = read_csv("./data/solubility_train.csv")
test_data = read_csv("./data/solubility_test.csv")
train_data = na.omit(train_data)
test_data = na.omit(test_data)
set.seed(1993)
```

## Prompt

In this exercise, we will predict solubility of compounds using their chemical structures.

The training data are in the file “solubility train.csv” and the test data are in “solubil-
ity test.csv”. Among the 228 predictors, 208 are binary variables that indicate the presence

or absence of a particular chemical substructure, 16 are count features, such as the number
of bonds or the number of bromine atoms, and 4 are continuous features, such as molecular
weight or surface area. The response is in the column “Solubility” (the last column).

## Questions

(A) Fit a linear model using least squares on the training data and calculate the mean squared error using the test data.

(B) Fit a ridge regression model on the training data, with λ chosen by cross-validation. Report the test error.

(C) Fit a lasso model on the training data, with λ chosen by cross-validation. Report the test error and the number of non-zero coefficient estimates in your model.

(D) Fit a principle component regression model on the training data, with M chosen by cross-validation. Report the test error and the value of M selected by cross-validation.

(E) Which model will you choose for predicting solubility?

# Section 2: Implementation and Results

## Problem A)
The dataframe consists of 229 variables. There are a large number of predictors and potential collinearity (implying large variance), but a linear model is implemented to start. While it would therefore be computationally unwise to compute all or best subsets and choose that way, this approach is included for sake of completeness

Note that in performing cross-validation, you want to apply CV both when finding the predictors with the largest correlation with the response; and, you want to do it when fitting the model using only those predictors. 

```{r explore}
# summary(train_data)
```

```{r best subsets, eval = F}
regsubsets_obj = regsubsets(Solubility ~ ., data = train_data, method = "backward", nbest = 1)
# summary(regsubsetsObj)
plot(regsubsets_obj, scale = "bic")
```

We train the model using cross-validation.
```{r lm, cache = T, message=F}
set.seed(1993)
control_cv <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

# lm(Solubility ~ ., data = train_data)

fit.lm <- train(Solubility~.,
          data = train_data,
          method = "lm",
          trControl = control_cv)

# fit.lm$finalModel
```

Then, we apply the model to the test data and obtain the MSE for evaluation purposes.
```{r}
#Fitting training model on test set
pred.lm = predict(fit.lm, newdata = test_data)
rmse_lm = RMSE(pred.lm, test_data$Solubility)
rmse_lm
```

## Problem B)
(B) Fit a ridge regression model on the training data, with λ chosen by cross-validation. Report the test error.

Since regularization methods like ridge regression is not scale-invariant like LS methods (since ridge regression minimizes the objective function which includes a penalty term) so we must standardize our predictors first.
```{r ridge, cache = T}
train_data_model_matrix <- model.matrix(Solubility ~ ., train_data)[ ,-1]

control_ridge <- trainControl(method = "repeatedcv", number = 10, repeats = 5)

set.seed(1993)
ridge.fit <- train(train_data_model_matrix, train_data$Solubility,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 0, 
                                          lambda = exp(seq(7, -3, length=100))),
                   # preProc = c(scale), glmnet does this by default
                   trControl = control_ridge)

plot(ridge.fit, xTrans = log)

ridge.fit$bestTune

# coefficients in the final model
coef(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda)
```

The optimal value of lambda chosen by repeated cross-validation is `r ridge.fit$bestTune$lambda`. 









## Section 3: Model Comparison Using MSE
Using Resample to Compare Models
```{r}
resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, ridge = ridge.fit, lm = lm.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```























